{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision import transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "from snip import SNIP\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "LOG_INTERVAL = 20\n",
    "INIT_LR = 0.1\n",
    "WEIGHT_DECAY_RATE = 0.0005\n",
    "EPOCHS = 5\n",
    "REPEAT_WITH_DIFFERENT_SEED = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def apply_prune_mask(net, keep_masks):\n",
    "\n",
    "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
    "    # one-to-one by removing all the irrelevant modules:\n",
    "    prunable_layers = filter(\n",
    "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
    "            layer, nn.Linear), net.modules())\n",
    "\n",
    "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
    "        assert (layer.weight.shape == keep_mask.shape)\n",
    "\n",
    "        def hook_factory(keep_mask):\n",
    "            \"\"\"\n",
    "            The hook function can't be defined directly here because of Python's\n",
    "            late binding which would result in all hooks getting the very last\n",
    "            mask! Getting it through another function forces early binding.\n",
    "            \"\"\"\n",
    "\n",
    "            def hook(grads):\n",
    "                return grads * keep_mask\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # mask[i] == 0 --> Prune parameter\n",
    "        # mask[i] == 1 --> Keep parameter\n",
    "\n",
    "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
    "        # Step 2: Make sure their gradients remain zero\n",
    "        layer.weight.data[keep_mask == 0.] = 0.\n",
    "        layer.weight.register_hook(hook_factory(keep_mask))\n",
    "\n",
    "\n",
    "class LeNet_300_100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "class LeNet_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc3 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc4 = nn.Linear(120, 84)\n",
    "        self.fc5 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.fc3(x.view(-1, 16 * 5 * 5)))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.log_softmax(self.fc5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeNet_5_Caffe(nn.Module):\n",
    "    \"\"\"\n",
    "    This is based on Caffe's implementation of Lenet-5 and is slightly different\n",
    "    from the vanilla LeNet-5. Note that the first layer does NOT have padding\n",
    "    and therefore intermediate shapes do not match the official LeNet-5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, padding=0)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5)\n",
    "        self.fc3 = nn.Linear(50 * 4 * 4, 500)\n",
    "        self.fc4 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.fc3(x.view(-1, 50 * 4 * 4)))\n",
    "        x = F.log_softmax(self.fc4(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "VGG_CONFIGS = {\n",
    "    # M for MaxPool, Number for channels\n",
    "    'D': [\n",
    "        64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M',\n",
    "        512, 512, 512, 'M'\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG_SNIP(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a base class to generate three VGG variants used in SNIP paper:\n",
    "        1. VGG-C (16 layers)\n",
    "        2. VGG-D (16 layers)\n",
    "        3. VGG-like\n",
    "\n",
    "    Some of the differences:\n",
    "        * Reduced size of FC layers to 512\n",
    "        * Adjusted flattening to match CIFAR-10 shapes\n",
    "        * Replaced dropout layers with BatchNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = self.make_layers(VGG_CONFIGS[config], batch_norm=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),  # 512 * 7 * 7 in the original VGG\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),  # instead of dropout\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),  # instead of dropout\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def make_layers(config, batch_norm=False):  # TODO: BN yes or no?\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in config:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [\n",
    "                        conv2d,\n",
    "                        nn.BatchNorm2d(v),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    ]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_mnist_dataloaders(train_batch_size, val_batch_size):\n",
    "\n",
    "    data_transform = Compose([transforms.ToTensor()])\n",
    "\n",
    "    # Normalise? transforms.Normalize((0.1307,), (0.3081,))\n",
    "\n",
    "    train_dataset = MNIST(\"_dataset\", True, data_transform, download=True)\n",
    "    test_dataset = MNIST(\"_dataset\", False, data_transform, download=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def get_cifar10_dataloaders(train_batch_size, test_batch_size):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10('_dataset', True, train_transform, download=True)\n",
    "    test_dataset = CIFAR10('_dataset', False, test_transform, download=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def mnist_experiment():\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "    LR_DECAY_INTERVAL = 25000\n",
    "\n",
    "    net = LeNet_300_100().to(device)\n",
    "    # net = LeNet_5()\n",
    "#     net = LeNet_5_Caffe().to(device)\n",
    "\n",
    "    optimiser = optim.SGD(\n",
    "        net.parameters(),\n",
    "        lr=INIT_LR,\n",
    "        momentum=0.9,\n",
    "        weight_decay=WEIGHT_DECAY_RATE)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimiser, 30000, gamma=0.1)\n",
    "\n",
    "    train_loader, val_loader = get_mnist_dataloaders(BATCH_SIZE, BATCH_SIZE)\n",
    "\n",
    "    return net, optimiser, lr_scheduler, train_loader, val_loader\n",
    "\n",
    "\n",
    "def cifar10_experiment():\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LR_DECAY_INTERVAL = 30000\n",
    "\n",
    "    net = VGG_SNIP('D').to(device)\n",
    "\n",
    "    optimiser = optim.SGD(\n",
    "        net.parameters(),\n",
    "        lr=INIT_LR,\n",
    "        momentum=0.9,\n",
    "        weight_decay=WEIGHT_DECAY_RATE)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimiser, LR_DECAY_INTERVAL, gamma=0.1)\n",
    "\n",
    "    train_loader, val_loader = get_cifar10_dataloaders(BATCH_SIZE,\n",
    "                                                       BATCH_SIZE)  # TODO\n",
    "\n",
    "    return net, optimiser, lr_scheduler, train_loader, val_loader\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    net, optimiser, lr_scheduler, train_loader, val_loader = mnist_experiment()\n",
    "\n",
    "    # Pre-training pruning using SKIP\n",
    "    keep_masks = SNIP(net, 1.0, train_loader, device)  # TODO: shuffle?\n",
    "    apply_prune_mask(net, keep_masks)\n",
    "\n",
    "    trainer = create_supervised_trainer(net, optimiser, F.nll_loss, device)\n",
    "    evaluator = create_supervised_evaluator(net, {\n",
    "        'accuracy': Accuracy(),\n",
    "        'nll': Loss(F.nll_loss)\n",
    "    }, device)\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    pbar.attach(trainer)\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_loss(engine):\n",
    "        lr_scheduler.step()\n",
    "        iter_in_epoch = (engine.state.iteration - 1) % len(train_loader) + 1\n",
    "        if engine.state.iteration % LOG_INTERVAL == 0:\n",
    "#             pbar.log_message(\"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "#                   \"\".format(engine.state.epoch, iter_in_epoch, len(train_loader), engine.state.output))\n",
    "            writer.add_scalar(\"training/loss\", engine.state.output,\n",
    "                              engine.state.iteration)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_epoch(engine):\n",
    "        evaluator.run(val_loader)\n",
    "\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics['accuracy']\n",
    "        avg_nll = metrics['nll']\n",
    "\n",
    "        pbar.log_message(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "              .format(engine.state.epoch, avg_accuracy, avg_nll))\n",
    "\n",
    "        writer.add_scalar(\"validation/loss\", avg_nll, engine.state.iteration)\n",
    "        writer.add_scalar(\"validation/accuracy\", avg_accuracy,\n",
    "                          engine.state.iteration)\n",
    "\n",
    "    trainer.run(train_loader, EPOCHS)\n",
    "\n",
    "    # Let's look at the final weights\n",
    "    # for name, param in net.named_parameters():\n",
    "    #     if name.endswith('weight'):\n",
    "    #         writer.add_histogram(name, param)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     for _ in range(REPEAT_WITH_DIFFERENT_SEED):\n",
    "#         train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2662, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "net, optimiser, lr_scheduler, train_loader, val_loader = mnist_experiment()\n",
    "\n",
    "# Pre-training pruning using SKIP\n",
    "keep_masks = SNIP(net, 0.01, train_loader, device)  # TODO: shuffle?\n",
    "apply_prune_mask(net, keep_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.array(list(net.modules())[1].weight.cpu().detach()).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params[params!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([67., 57., 49., 52., 54., 43., 60., 57., 52., 59., 51., 56., 59.,\n",
       "        45., 41., 50., 67., 52., 65., 45.]),\n",
       " array([-3.56193781e-02, -3.20556499e-02, -2.84919236e-02, -2.49281973e-02,\n",
       "        -2.13644709e-02, -1.78007446e-02, -1.42370164e-02, -1.06732901e-02,\n",
       "        -7.10956287e-03, -3.54583608e-03,  1.78907067e-05,  3.58161749e-03,\n",
       "         7.14534428e-03,  1.07090715e-02,  1.42727979e-02,  1.78365260e-02,\n",
       "         2.14002524e-02,  2.49639787e-02,  2.85277050e-02,  3.20914313e-02,\n",
       "         3.56551595e-02], dtype=float32),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(params[params!=0], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011775229"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(params[params!=0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
